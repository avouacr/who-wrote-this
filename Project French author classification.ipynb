{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP starting kit : Who Wrote This ?\n",
    "\n",
    "_Authors: Romain AVOUAC, Adrien DANEL, Guillaume DESFORGES, Jaime COSTA, José-Louis HIMBERT, Slimane THABET_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : ajouter l'illustration wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Introduction](#Introduction)\n",
    "1. [Data](#Data)\n",
    "2. [Score metric](#Score-metric)\n",
    "3. [Library requirements](#Library-requirements)\n",
    "4. [Basic text preprocessing](#Basic-text-preprocessing)\n",
    "5. [Exploratory analysis](#Exploratory-analysis)\n",
    "6. [Predictions](#Predictions)\n",
    "6. [Ramp workflow](#Ramp-workflow)\n",
    "9. [More information](#More-information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Authorship identification is the task of recognizing who the author of a document is.\n",
    "It is part of the Natural Language Processing (NLP) kind of tasks.\n",
    "\n",
    "Being able to identify the author of a document presents several applications, such as detecting plagiarism or finding the author of an anonymous document.\n",
    "Archives all around the world are full of documents for which knowing the author would be invaluable knowledge for historical studies.\n",
    "Furthermore, [the multiple plagiarism scandals](https://lithub.com/12-literary-plagiarism-scandals-ranked/) in literature could be solved with an algorithm.\n",
    "For instance the authorship of Moliere or Shakespeare has been debated from the 19th century (more on this [here](https://fr.wikipedia.org/wiki/Paternit%C3%A9_des_%C5%93uvres_de_Moli%C3%A8re) and [here](https://fr.wikipedia.org/wiki/Paternit%C3%A9_des_%C5%93uvres_de_Shakespeare)).\n",
    "\n",
    "This task has also an instructive purpose.\n",
    "It is a way to investigate if NLP algorithms are able to capture bot only the semantics, but also the literary style of a document.\n",
    "\n",
    "## Data\n",
    "\n",
    "We will limit ourselves to a selection of French novelists from the 19th century.\n",
    "\n",
    "**TODO**:\n",
    "* pourquoi le 19eme ?\n",
    "* quels auteurs ? pourquoi ?\n",
    "* quels textes ? pourquoi ?\n",
    "\n",
    "**TODO** lister\n",
    "* les fichiers\n",
    "* ce à quoi ils servent\n",
    "* leurs colonnes et la signification\n",
    "\n",
    "## Score metric\n",
    "\n",
    "We want to predict the author of a given piece of litterature from a given set of authors.\n",
    "In machine learning, this type of problem is called \"multiclass classification\" problem, that is for each item we predict the class that it belongs to.\n",
    "Here, the items are the documents (one or more paragraphs) and the classes are the authors.\n",
    "\n",
    "In order to evaluate the performance of an algorithm solving this type of problems, one could propose the *precision* of the algorithm.\n",
    "The precision of an algorithm in the prediction of a given class is defined as the number of right predictions on that class divided by the number of items where the algorithm predicted it.\n",
    "Then we could compute for instance the mean precision of the algorithm on all the classes.\n",
    "\n",
    "On the other hand, one could say that the *recall* of the algorithm is also important, or its *accuracy*.\n",
    "\n",
    "Most of the time, an algorithm can be tweaked to offer a better precision or a better recall, but not both at the same time - there is no free lunch.\n",
    "There is a tradeoff to be made, usually depending on the application domain.\n",
    "For example, in a medical team you would want as little false negatives.\n",
    "\n",
    "In order to evaluate the model, we propose to use the F1 metric.\n",
    "\n",
    "**TODO** continuer en présentant la F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library requirements\n",
    "\n",
    "To run this starting kit, the following libraries are required : \n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `nltk`\n",
    "- `plotly`\n",
    "- `plotly_express`\n",
    "- `matplotlib`\n",
    "- `seaborn`\n",
    "- `scikit-learn`\n",
    "\n",
    "They can be installed all at once using the `requirements.txt` file with pip :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make submissions to the challenge, the `ramp-workflow` library is also needed. It can be installed from GitHub using pip :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/paris-saclay-cds/ramp-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic text preprocessing\n",
    "\n",
    "NLP data is special in the sense that it is unstructured.\n",
    "Structured data are tables where each item is a set of key-value pairs, each pair reflecting a feature of the item.\n",
    "In this challenge, each item is a document of natural text.\n",
    "Most algorithm can't process those raw text as a sequence of characters, and it is part of the job of a data scientist to design the proper data processing pipelines.\n",
    "\n",
    "Usually, it starts with a tokenization step where the document is cut into pieces, such as words.\n",
    "Transforming the data from a sequence of characters to a sequence of words can then help engineering actual features for each document.\n",
    "\n",
    "Below is a simple tokenization :\n",
    "\n",
    "**TODO** ajouter la tokenization\n",
    "\n",
    "**TODO** écrire quelques limites/suggestions d'amélioration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adrien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting meaning from words with the LDA\n",
    "\n",
    "LDA stands for Latent Dirichlet Allocation.\n",
    "It is a simple yet powerful model that has been used in NLP.\n",
    "\n",
    "**TODO** continuer à présenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO slimane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Using the previous studies, we can build a predictive pipeline that can learn to classify documents by authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "import problem\n",
    "\n",
    "X_df, y_array = problem.get_train_data(sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(strip_accents='ascii',\n",
    "                                          max_df=0.7)\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        self.vectorizer.fit(X_df['paragraph'])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        X_preprocessed = self.vectorizer.transform(X_df['paragraph'])\n",
    "        return X_preprocessed\n",
    "    \n",
    "feature_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.clf = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.clf.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.clf.predict(X).astype(int)\n",
    "        return y_pred\n",
    "    \n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 8.134555e-01 (+/- 2.941361e-03)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('feature_extractor', feature_extractor),\n",
    "    ('classifier', classifier)])\n",
    "\n",
    "cv = problem.get_cv(X_df, y_array)\n",
    "\n",
    "scores_Xdf = cross_val_score(clf, X_df, y_array, cv=cv, scoring='f1_micro', n_jobs=3)\n",
    "\n",
    "print(\"mean: %e (+/- %e)\" % (scores_Xdf.mean(), scores_Xdf.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ramp workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each submission should be in it's own folder within the `submissions` folder (e.g. `submissions/my_submission`). The submission directory should contain 2 files:\n",
    "\n",
    "* `feature_extractor.py` - this should implement a feature extractor with `fit()` and `transform()` methods\n",
    "* `classifier.py` - this should implement a classifier with `fit()` and `predict()` methods\n",
    "\n",
    "See `submissions/starting_kit` for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local testing (before submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ramp-workflow` library provides a unit test - `ramp_test_submission` - to check whether a submission works locally before submitting it to the server. This command will test on files in [`submissions/starting_kit`](/submissions/starting_kit) by default. To specify testing on a different folder use the flag `--submission`. For example to run the test on `submissions/solution1` use: `ramp_test_submission --submission solution1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --submission starting_kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the README of the [ramp-workflow](https://github.com/paris-saclay-cds/ramp-workflow) library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
