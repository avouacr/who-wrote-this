{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP starting kit : Who Wrote This ?\n",
    "\n",
    "_Authors: Romain AVOUAC, Adrien DANEL, Guillaume DESFORGES, Jaime COSTA, José-Louis HIMBERT, Slimane THABET_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Introduction](#Introduction)\n",
    "1. [Data](#Data)\n",
    "2. [Score metric](#Score-metric)\n",
    "3. [Requirements](#Requirements)\n",
    "4. [Data exploration](#Data-exploration)\n",
    "5. [Predictions](#Predictions)\n",
    "6. [Submission structure](#Submission-structure)\n",
    "7. [Local testing](#Local-testing-(before-submission))\n",
    "8. [Submitting to RAMP studio](#Submitting-to-[ramp.studio](http://ramp.studio))\n",
    "9. [More information](#More-information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Authorship identification is the task of recognizing who the author of a document is.\n",
    "It is part of the Natural Language Processing (NLP) kind of tasks.\n",
    "\n",
    "Being able to identify the author of a document presents several applications, such as detecting plagiarism or finding the author of an anonymous document.\n",
    "Archives all around the world are full of documents for which knowing the author would be invaluable knowledge for historical studies.\n",
    "Furthermore, [the multiple plagiarism scandals](https://lithub.com/12-literary-plagiarism-scandals-ranked/) in literature could be solved with an algorithm.\n",
    "For instance the authorship of Moliere or Shakespeare has been debated from the 19th century (more on this [here](https://fr.wikipedia.org/wiki/Paternit%C3%A9_des_%C5%93uvres_de_Moli%C3%A8re) and [here](https://fr.wikipedia.org/wiki/Paternit%C3%A9_des_%C5%93uvres_de_Shakespeare)).\n",
    "\n",
    "This task has also an instructive purpose.\n",
    "It is a way to investigate if NLP algorithms are able to capture bot only the semantics, but also the literary style of a document.\n",
    "\n",
    "## Data\n",
    "\n",
    "We will limit ourselves to a selection of French novelists from the 19th century.\n",
    "\n",
    "**TODO**:\n",
    "* pourquoi le 19eme ?\n",
    "* quels auteurs ? pourquoi ?\n",
    "* quels textes ? pourquoi ?\n",
    "* ajouter l'image (wordcloud)\n",
    "\n",
    "**TODO** lister\n",
    "* les fichiers\n",
    "* ce à quoi ils servent\n",
    "* leurs colonnes et la signification\n",
    "\n",
    "## Score metric\n",
    "\n",
    "We want to predict the author of a given piece of litterature from a given set of authors.\n",
    "In machine learning, this type of problem is called \"multiclass classification\" problem, that is for each item we predict the class that it belongs to.\n",
    "Here, the items are the documents (one or more paragraphs) and the classes are the authors.\n",
    "\n",
    "In order to evaluate the performance of an algorithm solving this type of problems, one could propose the *precision* of the algorithm.\n",
    "The precision of an algorithm in the prediction of a given class is defined as the number of right predictions on that class divided by the number of items where the algorithm predicted it.\n",
    "Then we could compute for instance the mean precision of the algorithm on all the classes.\n",
    "\n",
    "On the other hand, one could say that the *recall* of the algorithm is also important, or its *accuracy*.\n",
    "\n",
    "Most of the time, an algorithm can be tweaked to offer a better precision or a better recall, but not both at the same time - there is no free lunch.\n",
    "There is a tradeoff to be made, usually depending on the application domain.\n",
    "For example, in a medical team you would want as little false negatives.\n",
    "\n",
    "In order to evaluate the model, we propose to use the F1 metric.\n",
    "\n",
    "**TODO** continuer en présentant la F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "To run this starting kit, the following libraries are required : \n",
    "- numpy\n",
    "- pandas\n",
    "- nltk\n",
    "- plotly\n",
    "- plotly_express\n",
    "- matplotlib\n",
    "- seaborn\n",
    "\n",
    "They can be installed all at once using the requirements.txt file with pip :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make submissions to the challenge, the ramp-workflow library is also needed. It can be installed from GitHub using pip :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/paris-saclay-cds/ramp-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic NLP Data Processing\n",
    "\n",
    "NLP data is special in the sense that it is unstructured.\n",
    "Structured data are tables where each item is a set of key-value pairs, each pair reflecting a feature of the item.\n",
    "In this challenge, each item is a document of natural text.\n",
    "Most algorithm can't process those raw text as a sequence of characters, and it is part of the job of a data scientist to design the proper data processing pipelines.\n",
    "\n",
    "Usually, it starts with a tokenization step where the document is cut into pieces, such as words.\n",
    "Transforming the data from a sequence of characters to a sequence of words can then help engineering actual features for each document.\n",
    "\n",
    "Below is a simple tokenization :\n",
    "\n",
    "**TODO** ajouter la tokenization\n",
    "\n",
    "**TODO** écrire quelques limites/suggestions d'amélioration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO adrien"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting meaning from words with the LDA\n",
    "\n",
    "LDA stands for Latent Dirichlet Allocation.\n",
    "It is a simple yet powerful model that has been used in NLP.\n",
    "\n",
    "**TODO** continuer à présenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO slimane"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The challenge\n",
    "\n",
    "Using the previous studies, we can build a predictive pipeline that can learn to classify documents by authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO FeatureExtractor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ROC AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO crossval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local testing with RAMP (before submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --submission starting_kit --quick-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the README of the [ramp-workflow](https://github.com/paris-saclay-cds/ramp-workflow) library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
