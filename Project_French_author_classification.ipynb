{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAMP starting kit : Who Wrote This ?\n",
    "\n",
    "_Authors: Romain AVOUAC, Jaime COSTA, Adrien DANEL, Guillaume DESFORGES, José-Louis IMBERT, Slimane THABET_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** : ajouter l'illustration wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "0. [Introduction](#Introduction)\n",
    "1. [Data](#Data)\n",
    "2. [Score metric](#Score-metric)\n",
    "3. [Library requirements](#Library-requirements)\n",
    "4. [Basic text preprocessing](#Basic-text-preprocessing)\n",
    "5. [Exploratory analysis](#Exploratory-analysis)\n",
    "6. [Predictions](#Predictions)\n",
    "6. [Ramp workflow](#Ramp-workflow)\n",
    "9. [More information](#More-information)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Authorship identification is the task of recognizing who the author of a document is.\n",
    "It is part of the Natural Language Processing (NLP) kind of tasks.\n",
    "\n",
    "Being able to identify the author of a document presents several applications, such as detecting plagiarism or finding the author of an anonymous document.\n",
    "Archives all around the world are full of documents for which knowing the author would be invaluable knowledge for historical studies.\n",
    "Furthermore, [the multiple plagiarism scandals](https://lithub.com/12-literary-plagiarism-scandals-ranked/) in literature could be solved with an algorithm.\n",
    "For instance the authorship of Moliere or Shakespeare has been debated from the 19th century (more on this [here](https://fr.wikipedia.org/wiki/Paternit%C3%A9_des_%C5%93uvres_de_Moli%C3%A8re) and [here](https://fr.wikipedia.org/wiki/Paternit%C3%A9_des_%C5%93uvres_de_Shakespeare)).\n",
    "\n",
    "This task has also an instructive purpose.\n",
    "It is a way to investigate if NLP algorithms are able to capture bot only the semantics, but also the literary style of a document.\n",
    "\n",
    "## Data\n",
    "\n",
    "We will limit ourselves to a selection of French novelists from the 19th century. The idea of this challenge is to see if an algorithm can identify some literary style for determining the author. In the general case, it can be very easy to differentiate documents on the genre e.g between novels, poems, and plays. The same is true for the period of writing, one can easily differentiate 17th century and 20th century style. Therefore, we decided to select authors from the same literary genre and the same period. The following authors were selected in a attempt to be representative of the different styles of novels written in the 19th century:\n",
    "\n",
    "\n",
    "- Balzac\n",
    "- Daudet\n",
    "- Dumas\n",
    "- Hugo\n",
    "- Flaubert\n",
    "- Maupassant\n",
    "- Stendhal\n",
    "- Verne\n",
    "- Vigny\n",
    "- Zola\n",
    "\n",
    "For each author, 3 books have been selected with 2 for training and 1 for test. Each book has been cut into different paragraphs.\n",
    "\n",
    "\n",
    "\n",
    "**TODO**:\n",
    "* pourquoi le 19eme ?\n",
    "* quels auteurs ? pourquoi ?\n",
    "* quels textes ? pourquoi ?\n",
    "\n",
    "**TODO** lister\n",
    "* les fichiers\n",
    "* ce à quoi ils servent\n",
    "* leurs colonnes et la signification\n",
    "\n",
    "## Score metric\n",
    "\n",
    "We want to predict the author of a given piece of litterature from a given set of authors.\n",
    "In machine learning, this type of problem is called \"multiclass classification\" problem, that is for each item we predict the class that it belongs to.\n",
    "Here, the items are the documents (one or more paragraphs) and the classes are the authors.\n",
    "\n",
    "In order to evaluate the performance of an algorithm solving this type of problems, one could propose the *precision* of the algorithm.\n",
    "The precision of an algorithm in the prediction of a given class is defined as the number of right predictions on that class divided by the number of items where the algorithm predicted it.\n",
    "Then we could compute for instance the mean precision of the algorithm on all the classes.\n",
    "\n",
    "On the other hand, one could say that the *recall* of the algorithm is also important, or its *accuracy*.\n",
    "\n",
    "Most of the time, an algorithm can be tweaked to offer a better precision or a better recall, but not both at the same time - there is no free lunch.\n",
    "There is a tradeoff to be made, usually depending on the application domain.\n",
    "For example, in a medical team you would want as little false negatives.\n",
    "\n",
    "In order to evaluate the model, we propose to use the F1 metric.\n",
    "\n",
    "**TODO** continuer en présentant la F1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library requirements\n",
    "\n",
    "To run this starting kit, the following libraries are required : \n",
    "- `numpy`\n",
    "- `pandas`\n",
    "- `nltk`\n",
    "- `plotly`\n",
    "- `plotly_express`\n",
    "- `matplotlib`\n",
    "- `seaborn`\n",
    "- `scikit-learn`\n",
    "- `gensim`\n",
    "\n",
    "They can be installed all at once using the `requirements.txt` file with pip :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make submissions to the challenge, the `ramp-workflow` library is also needed. It can be installed from GitHub using pip :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/paris-saclay-cds/ramp-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"\",sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic text preprocessing\n",
    "\n",
    "NLP data is special in the sense that it is unstructured.\n",
    "Structured data are tables where each item is a set of key-value pairs, each pair reflecting a feature of the item.\n",
    "In this challenge, each item is a document of natural text.\n",
    "Most algorithm can't process those raw text as a sequence of characters, and it is part of the job of a data scientist to design the proper data processing pipelines.\n",
    "\n",
    "Usually, it starts with a tokenization step where the document is cut into pieces, such as words.\n",
    "Transforming the data from a sequence of characters to a sequence of words can then help engineering actual features for each document.\n",
    "\n",
    "Below is a simple tokenization :\n",
    "\n",
    "**TODO** ajouter la tokenization\n",
    "\n",
    "**TODO** écrire quelques limites/suggestions d'amélioration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic features and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, download\n",
    "from re import split\n",
    "from numpy import mean\n",
    "\n",
    "download('punkt', quiet=True)\n",
    "\n",
    "_punctuation = '.?!:;&()`\"\\'@°_-~'\n",
    "\n",
    "def _getWordFeatures(tokens):\n",
    "\n",
    "    tokensLengths = [len(t) for t in tokens]\n",
    "    return mean(tokensLengths), max(tokensLengths)\n",
    "\n",
    "def _buildSentencesFeatures(text):\n",
    "\n",
    "    sentences = split(pattern='\\\\. |\\\\! |\\\\? ', string=text)\n",
    "    sentencesLengths = [len(word_tokenize(s)) for s in sentences]\n",
    "\n",
    "    return len(sentences), mean(sentencesLengths), max(sentencesLengths)\n",
    "\n",
    "def _getWordsWithNumbersCount(tokens):\n",
    "    return sum([any(c.isdigit() for c in t) for t in tokens])\n",
    "\n",
    "def _getWordsWithPunctuationCount(tokens):\n",
    "    return sum([any(c in _punctuation for c in t) for t in tokens])\n",
    "\n",
    "def _getCommonWordsCount(tokens):\n",
    "    return sum([t in _commonWords for t in tokens])\n",
    "\n",
    "def _getCommonWordsAverageFrequency(tokens):\n",
    "    return mean([_commonWords[t] if t in _commonWords else 0.0 for t in tokens])\n",
    "                          \n",
    "def _analyze_single(text):\n",
    "\n",
    "    features = {}\n",
    "\n",
    "    # Sentences Features\n",
    "    features['numberOfSentences'], features['meanSentenceLength'], features['maxSentenceLength'] = _buildSentencesFeatures(text)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    percentage = int(100000 / len(tokens)) / 1000\n",
    "\n",
    "    # Word based features\n",
    "    features['wordCount'] = len(tokens)\n",
    "    features['meanWordLength'], features['maxWordLength'] = _getWordFeatures(tokens)\n",
    "\n",
    "    # Specific tokens based features\n",
    "    features['wordsWithNumbersPercentage'] = _getWordsWithNumbersCount(tokens) * percentage\n",
    "    features['wordsWithPunctuationPercentage'] = _getWordsWithPunctuationCount(tokens) * percentage\n",
    "\n",
    "    del tokens\n",
    "\n",
    "    return features\n",
    "\n",
    "df_feats = df['paragraph'].apply(lambda x : _analyze_single(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** si on garde cette partie, l'améliorer car dégueu actuellement (**AD**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_nof = []\n",
    "l_wordcount = []\n",
    "l_meanWordLength = []\n",
    "l_maxWordLength = []\n",
    "l_wordsWithNumbersPercentage = []\n",
    "l_wordsWithPunctuationPercentage = []\n",
    "\n",
    "for i in range(len(df_feats)):\n",
    "    \n",
    "    l_nof.append(df_feats[i]['numberOfSentences'])\n",
    "    l_meanWordLength.append(df_feats[i]['meanWordLength'])\n",
    "    l_maxWordLength.append(df_feats[i]['maxWordLength'])\n",
    "    l_wordsWithNumbersPercentage.append(df_feats[i]['wordsWithNumbersPercentage'])\n",
    "    l_wordsWithPunctuationPercentage.append(df_feats[i]['wordsWithPunctuationPercentage'])\n",
    "    l_wordcount.append(df_feats[i]['wordCount'])\n",
    "\n",
    "df = df.assign(numberOfSentences = l_nof, wordCount = l_wordcount, meanWordLength = l_meanWordLength,\n",
    "               maxWordLength = l_maxWordLength, wordsWithNumbersPercentage = l_wordsWithNumbersPercentage,\n",
    "               wordsWithPunctuationPercentage = l_wordsWithPunctuationPercentage)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting meaning from words with the LDA\n",
    "\n",
    "LDA stands for Latent Dirichlet Allocation.\n",
    "It is a simple yet powerful model that has been used in NLP.\n",
    "\n",
    "It considers latent variables called topics which can take T values. The way of generating a document $d$ from the LDA model is the following :\n",
    "\n",
    "- Let $\\boldsymbol{\\alpha}_d = (\\alpha_{d1}, ..., \\alpha_{dT})  \\sim Dirichlet(\\alpha_0)$ be a vector of size T.\n",
    "- Each topic has a vector of probabilities $\\boldsymbol{\\beta}_t$ of size $|W|$ such that $\\beta_{tw}$ is the probabilities of the word $w$ given the topic $t$. For each word $w$ of $d$, draw a topic from the probabilities given by $\\boldsymbol{\\alpha}_d$ and draw a word from $\\boldsymbol{\\beta}_t$.\n",
    "\n",
    "In this case, fitting the models consists of determining the matrices $\\boldsymbol{\\alpha}$ and $\\boldsymbol{\\beta}$. For each author, we compute the average of the topic distribution of his documents, that gives us the topic distribution we can associate to him. \n",
    "\n",
    "**TODO** continuer à présenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jaime/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download(\"stopwords\")\n",
    "_punctuation = '.?!:;&()`\"\\'@°_-~'\n",
    "CUSTOM_STOPWORDS = [\"--\", \".\", \",\", \"!\", \";\", \"’\", \":\", \"?\", \"...\", \"'\", \"«\", \"»\", '(', ')', '[', ']']\n",
    "other_stopwords = ['comme', 'elles', \"c'était\", \"qu'il\", \"qu'elle\", 'où', 'car', 'sans', 'vers', 'encore', 'cette', \n",
    "                  'a', 'faire', 'fait', 'fais', 'à', 'donc', 'tout', 'cet', 'là', 'ceux', 'leur', 'leurs', 'parmi', \n",
    "                  'puis', 'ensuite', 'alors', \"qu'ils\", \"qu'elles\", \"m'en\", \"j'en\", 'dit-il', 'dit-elle', 'répondit',\n",
    "                  \"s'ils\", 'vont', \"s'il\", \"n'est\", 'pourquoi', \"lorsqu'il\", \"lorsqu'elle\", \"presque\", 'lorsque', \n",
    "                  \"contre\", 'toujours', 'plus', 'dès', 'autre', 'tous', 'tout', 'si', \"j'ai\", \"tous\", 'tout', 'toutes',\n",
    "                  'pourtant', \"c'est\", \"cela\", \"être\", \"jamais\", \"s'était\", \"l'avait\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_stopwords_remover(stopwords):\n",
    "    def stopwords_remover(words):\n",
    "        return [word for word in words if word not in stopwords]\n",
    "\n",
    "    return stopwords_remover\n",
    "\n",
    "def flatten_count(accumulator, items):\n",
    "    for item in items:\n",
    "        accumulator[item] = accumulator.get(item, 0) + 1\n",
    "    return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re # needed as re does not allow infinite lookbacks\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def extract_proper_noun(txt):\n",
    "    return re.findall(r\"(?<!^|\\. |\\.  )[A-Z][a-z]+\",txt)\n",
    "\n",
    "french_stopwords = set(stopwords.words(\"french\")).union(CUSTOM_STOPWORDS).union(other_stopwords)\n",
    "\n",
    "proper_noun_thresh=6\n",
    "proper_nouns = df.paragraph.apply(extract_proper_noun).apply(pd.Series).stack()\n",
    "counted = Counter(proper_nouns) \n",
    "filtered_proper_nouns = [el for el in proper_nouns if counted[el] >= proper_noun_thresh]\n",
    "filtered_proper_nouns = np.unique(filtered_proper_nouns)\n",
    "filtered_proper_nouns = [x.lower() for x in filtered_proper_nouns]\n",
    "\n",
    "words_to_remove = set(french_stopwords).union(set(filtered_proper_nouns))\n",
    "\n",
    "df[\"tokenized\"] = df[\"paragraph\"].str.lower().map(word_tokenize)\n",
    "df[\"tokenized\"] = df[\"tokenized\"].map(make_stopwords_remover(words_to_remove))\n",
    "docs = df[\"tokenized\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "dictionary = Dictionary(docs)\n",
    "dictionary.filter_extremes(no_below=20, no_above=0.5)\n",
    "\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_topics = np.zeros((len(corpus), num_topics))\n",
    "for i in range(documents_topics.shape[0]):\n",
    "    topics = model.__getitem__(corpus[i])\n",
    "    for t,p in topics:\n",
    "        documents_topics[i,t] = p\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "authors = df['author'].iloc[sample].values\n",
    "authors_list = ['Balzac', 'Daudet', 'Dumas', 'Hugo', 'Flaubert', 'Maupassant', 'Stendhal', 'Verne', 'Vigny', 'Zola']\n",
    "\n",
    "#We consider the average of topics distribution of documents for each author\n",
    "authors_topics = np.zeros((len(authors_list), num_topics))\n",
    "\n",
    "for i,author in enumerate(authors_list):\n",
    "    authors_topics[i,:] = np.mean(documents_topics[authors==author,:], axis=0)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "heatmap = ax.imshow(authors_topics, cmap=plt.cm.Blues)\n",
    "ax.set_xticks(np.arange(num_topics))\n",
    "ax.set_xticklabels(np.arange(num_topics) + 1)\n",
    "ax.set_yticks(np.arange(len(authors_list)))\n",
    "ax.set_yticklabels(authors_list)\n",
    "fig.colorbar(heatmap)\n",
    "fig.set_size_inches(11,7)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "\n",
    "model_vis_data = pyLDAvis.gensim.prepare(model, corpus, dictionary)\n",
    "pyLDAvis.display(model_vis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "\n",
    "Using the previous studies, we can build a predictive pipeline that can learn to classify documents by authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "import problem\n",
    "\n",
    "X_train, y_train = problem.get_train_data(sep='|')\n",
    "X_test, y_test = problem.get_test_data(sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class FeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer(strip_accents='ascii',\n",
    "                                          max_df=0.7)\n",
    "\n",
    "    def fit(self, X_df, y=None):\n",
    "        self.vectorizer.fit(X_df['paragraph'])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        X_preprocessed = self.vectorizer.transform(X_df['paragraph'])\n",
    "        return X_preprocessed\n",
    "    \n",
    "feature_extractor = FeatureExtractor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.classifier = LogisticRegression(solver='lbfgs', max_iter=1000,\n",
    "                                             multi_class='multinomial')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.classifier.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.classifier.predict(X).astype(int)\n",
    "        return y_pred\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        proba_pred = self.classifier.predict_proba(X)\n",
    "        return proba_pred\n",
    "    \n",
    "classifier = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rampwf.score_types.classifier_base import ClassifierBaseScoreType\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class F1Score(ClassifierBaseScoreType):\n",
    "    is_lower_the_better = False\n",
    "    minimum = 0.0\n",
    "    maximum = 1\n",
    "\n",
    "    def __init__(self, name=\"F1-score\", precision=2):\n",
    "        self.name = name\n",
    "        self.precision = precision\n",
    "\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        return f1_score(y_true, y_pred, average='micro')\n",
    "    \n",
    "scorer = F1Score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "    ('feature_extractor', feature_extractor),\n",
    "    ('classifier', classifier)])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = clf.predict(X_train)\n",
    "y_test_pred = clf.predict(X_test)\n",
    "\n",
    "train_score = scorer(y_true=y_train, y_pred=y_train_pred)\n",
    "test_score = scorer(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "print('F1 score on train set : ', train_score)\n",
    "print('F1 score on test set : ', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ramp workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each submission should be in it's own folder within the `submissions` folder (e.g. `submissions/my_submission`). The submission directory should contain 2 files:\n",
    "\n",
    "* `feature_extractor.py` - this should implement a feature extractor with `fit()` and `transform()` methods\n",
    "* `classifier.py` - this should implement a classifier with `fit()` and `predict_proba()` methods\n",
    "\n",
    "See `submissions/starting_kit` for an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local testing (before submission)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ramp-workflow` library provides a unit test - `ramp_test_submission` - to check whether a submission works locally before submitting it to the server. This command will test on files in [`submissions/starting_kit`](/submissions/starting_kit) by default. To specify testing on a different folder use the flag `--submission`. For example to run the test on `submissions/solution1` use: `ramp_test_submission --submission solution1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;178m\u001b[1mTesting Who wrote this? Predicting the author of a paragraph\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading train and test files from ./data ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mReading cv ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mTraining submissions/starting_kit ...\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 0\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore  F1-score       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m      \u001b[38;5;10m\u001b[1m0.91\u001b[0m  \u001b[38;5;150m46.961329\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m      \u001b[38;5;12m\u001b[1m0.82\u001b[0m   \u001b[38;5;105m3.172486\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m       \u001b[38;5;1m\u001b[1m0.48\u001b[0m   \u001b[38;5;218m1.174487\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 1\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore  F1-score       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m      \u001b[38;5;10m\u001b[1m0.91\u001b[0m  \u001b[38;5;150m44.838507\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m      \u001b[38;5;12m\u001b[1m0.81\u001b[0m   \u001b[38;5;105m3.544223\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m       \u001b[38;5;1m\u001b[1m0.49\u001b[0m   \u001b[38;5;218m1.447141\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 2\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore  F1-score       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m      \u001b[38;5;10m\u001b[1m0.91\u001b[0m  \u001b[38;5;150m49.533708\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m      \u001b[38;5;12m\u001b[1m0.81\u001b[0m   \u001b[38;5;105m3.491098\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m       \u001b[38;5;1m\u001b[1m0.48\u001b[0m   \u001b[38;5;218m1.336885\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 3\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore  F1-score       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m      \u001b[38;5;10m\u001b[1m0.91\u001b[0m  \u001b[38;5;150m47.044122\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m      \u001b[38;5;12m\u001b[1m0.81\u001b[0m   \u001b[38;5;105m3.691056\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m       \u001b[38;5;1m\u001b[1m0.48\u001b[0m   \u001b[38;5;218m1.259212\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 4\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore  F1-score       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m      \u001b[38;5;10m\u001b[1m0.91\u001b[0m  \u001b[38;5;150m44.738262\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m      \u001b[38;5;12m\u001b[1m0.82\u001b[0m   \u001b[38;5;105m3.830624\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m       \u001b[38;5;1m\u001b[1m0.48\u001b[0m   \u001b[38;5;218m1.242205\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mCV fold 5\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore  F1-score       time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m      \u001b[38;5;10m\u001b[1m0.91\u001b[0m  \u001b[38;5;150m41.554646\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m      \u001b[38;5;12m\u001b[1m0.82\u001b[0m   \u001b[38;5;105m3.501600\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m       \u001b[38;5;1m\u001b[1m0.48\u001b[0m   \u001b[38;5;218m1.317080\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mMean CV scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore      F1-score         time\u001b[0m\n",
      "\t\u001b[38;5;10m\u001b[1mtrain\u001b[0m  \u001b[38;5;10m\u001b[1m0.91\u001b[0m \u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m \u001b[38;5;150m0.001\u001b[0m  \u001b[38;5;150m45.8\u001b[0m \u001b[38;5;150m\u001b[38;5;150m±\u001b[0m\u001b[0m \u001b[38;5;150m2.48\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m  \u001b[38;5;12m\u001b[1m0.82\u001b[0m \u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m \u001b[38;5;105m0.005\u001b[0m    \u001b[38;5;105m3.5\u001b[0m \u001b[38;5;105m\u001b[38;5;105m±\u001b[0m\u001b[0m \u001b[38;5;105m0.2\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m   \u001b[38;5;1m\u001b[1m0.48\u001b[0m \u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m \u001b[38;5;218m0.001\u001b[0m   \u001b[38;5;218m1.3\u001b[0m \u001b[38;5;218m\u001b[38;5;218m±\u001b[0m\u001b[0m \u001b[38;5;218m0.09\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1mBagged scores\u001b[0m\n",
      "\u001b[38;5;178m\u001b[1m----------------------------\u001b[0m\n",
      "\t\u001b[38;5;178m\u001b[1mscore  F1-score\u001b[0m\n",
      "\t\u001b[38;5;12m\u001b[1mvalid\u001b[0m      \u001b[38;5;12m\u001b[1m0.82\u001b[0m\n",
      "\t\u001b[38;5;1m\u001b[1mtest\u001b[0m       \u001b[38;5;1m\u001b[1m0.48\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!ramp_test_submission --submission starting_kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More information\n",
    "\n",
    "You can find more information in the README of the [ramp-workflow](https://github.com/paris-saclay-cds/ramp-workflow) library."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
